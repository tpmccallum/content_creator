{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tpmccallum/content_creator/blob/main/content_creator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEZmjGQGW1ue"
      },
      "outputs": [],
      "source": [
        "!pip install kaleido\n",
        "!pip install langchain\n",
        "!pip install huggingface_hub\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install unstructured\n",
        "!pip install chromadb\n",
        "!pip install Cython\n",
        "!pip install tiktoken\n",
        "!pip install unstructured[local-inference]\n",
        "!pip install -qU transformers\n",
        "!pip install -qU accelerate\n",
        "!pip install -qU einops\n",
        "!pip install -qU xformers\n",
        "!pip install -qU bitsandbytes\n",
        "!pip install -qU faiss-gpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restart the runtime now\n",
        "\n",
        "Restart and re-run via the main menu in Collab i.e.\n",
        "\n",
        "Runtime -> Restart Runtime.\n",
        "\n",
        "Runtime -> Run All."
      ],
      "metadata": {
        "id": "9hTTR4WZTHSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "Ra7uy0RhXd_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import torch\n",
        "import requests\n",
        "import transformers\n",
        "from torch import cuda, bfloat16\n",
        "import xml.dom.minidom as minidom\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "8AZVtuZiW9Il"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Token\n",
        "Add your Hugging Face API key below"
      ],
      "metadata": {
        "id": "MS3El5kODQde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YourHuggingFaceAPIToken = 'abcd'"
      ],
      "metadata": {
        "id": "khk0Z-2xDPYO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtain web pages in use\n",
        "Fetch the sitemaps and pages from:\n",
        "- Fermyon corporate website\n",
        "- Fermyon developer documentation\n",
        "- Official WebAssembly Component Model documentation from the BytecodeAlliance"
      ],
      "metadata": {
        "id": "Lf6Qa7baeGq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch sitemap's text of each site that is to be indexed\n",
        "fermyon_website_sitemap = requests.get('https://www.fermyon.com/sitemap.xml', allow_redirects=True).text\n",
        "fermyon_documentation_sitemap = requests.get('https://developer.fermyon.com/sitemap.xml', allow_redirects=True).text\n",
        "component_model_documentation_sitemap = requests.get('https://component-model.bytecodealliance.org/sitemap.xml', allow_redirects=True).text\n",
        "\n",
        "# Parse each sitemap's text to obtain list of pages\n",
        "parsed_fermyon_website_sitemap_document = minidom.parseString(fermyon_website_sitemap)\n",
        "parsed_fermyon_documentation_sitemap_document = minidom.parseString(fermyon_documentation_sitemap)\n",
        "parsed_component_model_documentation_sitemap_document = minidom.parseString(component_model_documentation_sitemap)\n",
        "\n",
        "# Cherry pick just the loc elements from the XML\n",
        "fermyon_website_sitemap_loc_elements = parsed_fermyon_website_sitemap_document.getElementsByTagName('loc')\n",
        "fermyon_documentation_sitemap_loc_elements = parsed_fermyon_documentation_sitemap_document.getElementsByTagName('loc')\n",
        "component_model_documentation_sitemap_loc_elements = parsed_component_model_documentation_sitemap_document.getElementsByTagName('loc')\n",
        "\n",
        "# Declare blank lists of pages for each site\n",
        "fermyon_website_page_urls = []\n",
        "fermyon_documentation_page_urls = []\n",
        "component_model_documentation_page_urls = []\n",
        "\n",
        "# Iterate over loc elements (of each sitemap) and add to that site's list of pages\n",
        "for fermyon_website_sitemap_loc_element in fermyon_website_sitemap_loc_elements:\n",
        "    fermyon_website_page_urls.append(fermyon_website_sitemap_loc_element.toxml().removesuffix(\"</loc>\").removeprefix(\"<loc>\"))\n",
        "for fermyon_documentation_sitemap_loc_element in fermyon_documentation_sitemap_loc_elements:\n",
        "    fermyon_documentation_page_urls.append(fermyon_documentation_sitemap_loc_element.toxml().removesuffix(\"</loc>\").removeprefix(\"<loc>\"))\n",
        "for component_model_documentation_sitemap_loc_element in component_model_documentation_sitemap_loc_elements:\n",
        "    component_model_documentation_page_urls.append(component_model_documentation_sitemap_loc_element.toxml().removesuffix(\"</loc>\").removeprefix(\"<loc>\"))\n",
        "\n",
        "URLs = fermyon_website_page_urls + fermyon_documentation_page_urls + component_model_documentation_page_urls\n",
        "\n",
        "text_to_remove = \"rss\"\n",
        "filtered_list = [item for item in URLs if text_to_remove not in item]\n",
        "\n",
        "print(\"Number of page to process is {}\\n First page to process is {} and the last page to process is {}\".format(len(filtered_list), filtered_list[0], filtered_list[len(filtered_list) - 1]))"
      ],
      "metadata": {
        "id": "Ho_nzWd_vzUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = YourHuggingFaceAPIToken\n",
        "hf_auth = YourHuggingFaceAPIToken\n",
        "loader2 = [UnstructuredURLLoader(urls=filtered_list)]\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, you need an access token\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "!pip install accelerate\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "# enable evaluation mode to allow model inference\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "web_links = filtered_list\n",
        "loader = WebBaseLoader(web_links)\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "# storing embeddings in the vector store\n",
        "vectorstore = FAISS.from_documents(all_splits, embeddings)\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
      ],
      "metadata": {
        "id": "kdSWDCjSjRYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "Ask questions in the query variables below."
      ],
      "metadata": {
        "id": "5mm5kGLmE4zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "query1 = \"Can you please write me a few sentences about what Fermyon does and why a developer would want to use Fermyon Spin and Fermyon Cloud?\"\n",
        "result1 = chain({\"question\": query1, \"chat_history\": chat_history})\n",
        "\n",
        "print(\"Tell me about Fermyon?\\n\")\n",
        "print(result1['answer'])\n"
      ],
      "metadata": {
        "id": "DFGu-ThfmmUg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}